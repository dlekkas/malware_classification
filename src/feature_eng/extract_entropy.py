import sys
sys.path.insert(0, '../helpers')
from util import progress_bar, load_config, log_exception

from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
import pandas as pd
import numpy as np
import logging
import sys
import gzip
import scipy.stats

conf = load_config("../../config.yml")


def extract_byte_entropy_features(files_list, addrlength=32):
    ent_d_list = []
    for idx, file_name in enumerate(files_list):
        bytes_file = conf['dataset_dir'] + file_name + '.bytes.gz'
        bytes_seq = parse_bytes(bytes_file, addrlength)
        ent_stats = extract_entropy_statistics(bytes_seq, window_size=10000)
        ent_d_list.append(ent_stats)
        progress_bar(idx+1, len(files_list), 50)
    
    # convert list of dictionaries to pandas DataFrame
    vec = DictVectorizer()
    ent_features = vec.fit_transform(ent_d_list).toarray()
    ent_features = pd.DataFrame(ent_features, columns=vec.get_feature_names())
    # store entropy feature set in CSV file
    ent_features.to_csv('features/entropy_features.csv', index=False)
    return ent_features


def parse_bytes(file_name, addrlength=32):
    bytes_seq = bytearray()
    try:
        with gzip.open(file_name, 'rt') as fp:
            for line in fp.readlines():
                if not line.strip():
                    continue
                else:
                    mem_addr = int(addrlength/4)
                    line = line[mem_addr:].strip()
                    line = line.replace('?', '')  # ignore '?' characters
                    # store as bytearray for efficient memory management
                    bytes_seq = bytes_seq + bytearray.fromhex(line)
    except Exception as e:
        print(e)
        log_exception(e, sys.argv[0], file_name)
        bytes_seq = None
    return bytes_seq


def calc_byte_entropy(byte_seq, window_size):
    ents = []
    if not byte_seq:
        return [0]
    for i in range(0, len(byte_seq), window_size):
        # fixed size numpy array for optimized performance
        byte_count = np.zeros((256,), dtype=int)
        if len(byte_seq) < (i + window_size):
            for byte in byte_seq[i:len(byte_seq)]:
                byte_count[byte] += 1
        else:
            for byte in byte_seq[i:i+window_size]:
                byte_count[byte] += 1
        # calculate probability of occurence for each byte
        byte_probs = byte_count/window_size
        ents.append(scipy.stats.entropy(byte_probs))
    return ents


def extract_entropy_statistics(byte_array, window_size):
    ents = calc_byte_entropy(byte_array, window_size)    
    ent_statistics = {'ent_var': np.var(ents), 'ent_mean': np.mean(ents),
        'ent_P25': np.percentile(ents, 25), 'ent_P50': np.percentile(ents, 50) , 
        'ent_P75': np.percentile(ents, 75), 'ent_P95': np.percentile(ents, 95),
        'ent_total': calc_byte_entropy(byte_array, len(byte_array))[0]}
        #'total_bytes': len(byte_array)}
    return ent_statistics



train_labels_pd = pd.read_csv(conf['train_labels'])
files_list = train_labels_pd['Id'].tolist()
extract_byte_entropy_features(files_list)



'''
def shannon_byte_entropy(byte_array, window_size):
    # fixed size numpy array for optimized performance
    byte_count = np.zeros((256,), dtype=int)
    for byte in byte_seq:
        byte_count[byte] += 1
    # calculate probability of occurence for each byte
    byte_probs = byte_count / len(byte_seq)
    entropy = scipy.stats.entropy(byte_probs)
    return entropy
'''

