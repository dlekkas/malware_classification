from extract_xref import get_xref_features
from extract_xref import xref_initialization
from util import save_obj, load_obj, log_exception, progress_bar

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import log_loss


import scipy as sp
import pandas as pd
import numpy as np
import logging
import sys

DATASET_DIR = '/media/dimlek/TOSHIBA EXT/malware/'
LOGGING_FILE = 'log/features.log'

# Configure a custom logger for debugging purposes
logging.basicConfig(
        filename=LOGGING_FILE,
        format='%(asctime)s %(levelname)-8s %(message)s',
        level=logging.DEBUG,
        datefmt='%d-%m-%Y %H:%M:%S')


# Results of kaggle competition are evaluated using the logloss
# metric, hence this score metric will be used for gridsearchcv 
def svm_superparam_selection(X_train, X_test, y_train, y_test, nfolds, score_evals):
    tuned_params = {
            'C': [0.001, 0.01, 0.1, 1, 10],
            'kernel': ['rbf', 'sigmoid', 'poly'],
            'gamma': [0.001, 0.01, 0.1, 1]}
    bestscore_dict = {}
    for score in score_evals:
        grid_search_clf = GridSearchCV(SVC(), tuned_parameters, cv=nfolds, scoring=score)
        grid_search_clf.fit(X_train, y_train)
        print('# Tuning hyper-parameters for %s' % score)
        print('Best parameters found based on training set')
        print(grid_search_clf.best_params_)
        bestscore_dict[score] = grid_search_clf.best_params_
        mean_scores = grid_search_clf.cv_results_['mean_test_score']
        std_scores = grid_search_clf.cv_results_['std_test_score']
        print('Grid scores on training set:')
        for mean, std, params in zip(mean_scores, std_scores, grid_search_clf.cv_results_['params']):
            print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))

        # predict classifier's output
        print('Detailed classification report:')
        print('Scores based on full test set')
        y_pred = grid_search_clf.predict(X_test)
        print(classification_report(y_test, y_pred))
    
    return bestscore_dict



def main():
    train_labels = pd.read_csv(DATASET_DIR + 'trainLabels.csv')
    files_list = train_labels['Id'].tolist()
    
    # total number of files to calculate completion percentage
    total_files = len(files_list)

    # do not count corrupted files
    bad_files_idx = []
    bad_files_names = []
    # Extract all features related to DATA and CODE XREF
    xref_dict = xref_initialization()
    for idx, file_name in enumerate(files_list):
        asm_file = DATASET_DIR + 'train/' + file_name + '.asm.gz'
        try:
            get_xref_features(asm_file, xref_dict)
        except Exception as e:
            # log corrupted files for future correction
            log_exception(e, sys.argv[0], asm_file)
            bad_files_idx.append(idx)
            bad_files_names.append(file_name)

        progress_bar(idx+1, total_files, 50)

    xref_pd = pd.DataFrame.from_dict(xref_dict)
    
    # store xref features to avoid recalculation
    save_obj(xref_pd, 'xref_features')
    save_obj(bad_files_names, 'bad_files')

    # concat features with classes and IDs to create the dataset
    data = pd.concat([train_labels, xref_pd], axis=1, sort=False)

    # drop corrupted files (if any) from the training set
    if len(bad_files_idx) > 0:
        data.drop(data.index[bad_files_idx], inplace=True)
        data = data.reset_index(drop=True)
        # log the number of corrupted files
        logging.info('XREF Feature Extraction completed: ' + 
                str(len(bad_files_idx)) + ' file(s) are corrupted.')
        # store the corrupted files names in 'bad_asm_files.txt'
        with open('bad_asm_files.txt', 'w') as bfp:
            for name in bad_files_names:
                bfp.write(name + '.asm.gz')

    # save xref features dataframe to csv file to keep results (optional)
    data.to_csv('results/xref_features.csv')
    
    '''
    # create training and testing vars (80/20)
    X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2)
    
    # hyperparameter optimization for SVM classifier
    nfolds = 5
    scoring_evaluators = ['neg_log_loss']
    svm_superparam_selection(X_train, X_test, y_train, y_test, nfolds, scoring_evaluators)
    '''

if __name__ == "__main__":
    main()
