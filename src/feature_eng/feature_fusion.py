import sys
sys.path.insert(0, '../helpers')
from util import load_obj, load_config

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier 
from xgboost import XGBClassifier

from collections import OrderedDict
import pandas as pd
import numpy as np
import time
import yaml
import os

# load system configuration (i.e directory structure)
conf = load_config("../../config.yml")


# Forward stepwise selection was chosen as a suboptimal but tractable
# and computationally feasible algorithm to expose redundant features
# and select the optimal ones. Due to its greedy nature, an optimal
# feature selection is not guaranteed.
def fwd_stepwise_select(featsets_d, scoring, model, model_params, max_rounds=1, fd=sys.stdout):
    """
    Parameters
    ----------
    featsets_d: dict
        The dictionary, where key holds the feature-set name and value
        contains the DataFrame representing each distinct feature set
    scoring: str
        The scoring metric to evaluate model performance
    model: str
        The model that will evaluate the feature set; 'xgb' or 'rf'
    model_params: dict
        The dictionary of model's hyperparameters
    max_rounds: int
        Number of maximum rounds of greedy iterations
    
    Returns
    -------
    pandas.core.frame.DataFrame
        The union of features that yielded the highest score based on the  
        scoring metric supplied as argument
    """
    best_features = OrderedDict()
    best_score = -1
    new_score = 0
    while (new_score > best_score) and (max_rounds > 0):
        max_rounds = max_rounds - 1
        added_featset_name = None
        best_score = new_score
        for name, featset in featsets_d.items():
            best_features[name] = featset
            temp = featset_eval(best_features, scoring, model, model_params, 5, fd)
            # remove the added feature set until all are evaluated
            best_features.popitem()
            if temp >= new_score:
                new_score = temp
                added_featset_name = name

        if featsets_d and added_featset_name is not None:
            best_features[added_featset_name] = featsets_d[added_featset_name]
            del featsets_d[added_featset_name]

    # construct pandas DataFrame based on selected feature sets 
    selected_featset = pd.concat(list(best_features.values()), axis=1, sort=False)
    fd.write("Best Feature Set: " + "+".join(list(best_features.keys())) + '\n')
    fd.write("Best score: %0.3f\n" % (best_score))
    return selected_featset



def featset_eval(featset_d, scoring='accuracy', model='xgb', model_params=None, cv=5, fd=sys.stdout):
    labels = pd.read_csv(conf['train_labels'])
    data = pd.concat([labels] + list(featset_d.values()), axis=1, sort=False)
    y = data['Class']
    X = data.drop(['Class', 'Id'], axis=1)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    if model == 'rf':
        clf = RandomForestClassifier(**model_params, n_jobs=4)
    elif model == 'xgb':
        clf = XGBClassifier(**model_params, n_jobs=4)
    # TODO: assert that a valid model name is provided as argument
    cv_scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring=scoring)
    result = cv_scores.mean()
    fd.write("Feature Set: " + "+".join(featset_d.keys()) + "\n")
    fd.write("Accuracy: %0.3f (+/- %0.2f)\n" % (cv_scores.mean(), cv_scores.std() * 2))
    sys.exit(0)
    return result



# Collects all the feature sets represented as CSV files and located in the
# directory path supplied as argument. Returns a list of pandas DataFrames
# where each element represents a feature set in DataFrame format
def collect_feature_sets(features_dir):
    directory = os.fsencode(features_dir)
    featsets_d = {}
    for csv_file in os.listdir(directory):
        filename = os.fsdecode(csv_file)
        if filename.endswith(".csv"):
            # FIXME: replace the .iloc() invocation when fixed
            # it was added to sloppily patch shoddy to_csv()
            # invocations while extracting features
            featset = pd.read_csv(features_dir + filename).iloc[:,1:]
            # correlate each feature-set name with its pandas DataFrame
            featsets_d[os.path.splitext(filename)[0]] = featset
    return featsets_d



def run_experiment(scoring='accuracy', model='xgb', params_file=None, max_rounds=5):
    featsets_d = collect_feature_sets(conf['featsets_dir'])
    current_time = time.strftime("_%d.%m.%y_%H%M", time.localtime())
    res_file = conf['results_dir'] + 'res_' + model + current_time
    with open(params_file, 'r') as ymlfile:
        model_params = yaml.load(ymlfile)
    with open(res_file, 'w+') as fd:
        fd.write('----------- HYPERPARAMETERS -----------\n')
        fd.write(str(model_params) + '\n\n')
        fd.write('--------------- RESULTS ---------------\n')
        winner = fwd_stepwise_select(featsets_d, scoring, model, model_params, max_rounds, fd)
    return winner


exp_file = conf['xgbparams_dir'] + 'xgb_params.yaml'
run_experiment(scoring='accuracy', model='xgb', params_file=exp_file)

