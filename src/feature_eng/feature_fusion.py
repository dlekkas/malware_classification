import sys
sys.path.insert(0, '../helpers')
from util import load_obj, load_config

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier 
from xgboost import XGBClassifier

from collections import OrderedDict
import pandas as pd
import numpy as np
import time
import yaml
import os

# load system configuration (i.e directory structure)
conf = load_config("../../config.yml")


# Forward stepwise selection was chosen as a suboptimal but tractable
# and computationally feasible algorithm to expose redundant features
# and select the optimal ones. Due to its greedy nature, an optimal
# feature selection is not guaranteed.
def fwd_stepwise_select(featsets_d, model, scoring, max_rounds=1, fd=sys.stdout):
    """
    Parameters
    ----------
    featsets_d: dict
        The dictionary, where key holds the feature-set name and value
        contains the DataFrame representing each distinct feature set
    scoring: str
        The scoring metric to evaluate model performance
    model: str
        The model that will evaluate the feature set; 'xgb' or 'rf'
    max_rounds: int
        Number of maximum rounds of greedy iterations
    
    Returns
    -------
    pandas.core.frame.DataFrame
        The union of features that yielded the highest score based on the  
        scoring metric supplied as argument
    """
    best_features = OrderedDict()
    best_score = -1
    new_score = 0
    while (new_score > best_score) and (max_rounds > 0):
        max_rounds = max_rounds - 1
        added_featset_name = None
        best_score = new_score
        for name, featset in featsets_d.items():
            best_features[name] = featset
            temp = featset_eval(best_features, model, scoring, fd=fd)
            # remove the added feature set until all are evaluated
            best_features.popitem()
            if temp >= new_score:
                new_score = temp
                added_featset_name = name

        if featsets_d and added_featset_name is not None:
            best_features[added_featset_name] = featsets_d[added_featset_name]
            del featsets_d[added_featset_name]

    # construct pandas DataFrame based on selected feature sets 
    selected_featset = best_featset_report(best_features, model, scoring, fd=fd)
    return selected_featset



def featset_eval(featset_d, clf, scoring='accuracy', cv=5, fd=sys.stdout):
    labels = pd.read_csv(conf['train_labels'])
    data = pd.concat([labels] + list(featset_d.values()), axis=1, sort=False)
    y = data['Class']
    X = data.drop(['Class', 'Id'], axis=1)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    cv_scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring=scoring)
    result = cv_scores.mean()
    fd.write("Feature Set: " + "+".join(featset_d.keys()) + "\n")
    fd.write("Accuracy: {acc:.3f} % (+/- {std:.3f})\n\n".format(acc=cv_scores.mean()*100,
        std=cv_scores.std()*100))
    return result


def best_featset_report(featset_d, clf, scoring='accuracy', cv=5, fd=sys.stdout):
    labels = pd.read_csv(conf['train_labels'])
    data = pd.concat([labels] + list(featset_d.values()), axis=1, sort=False)
    y = data['Class']
    X = data.drop(['Class', 'Id'], axis=1)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)   
    cv_scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring=scoring)
    fd.write('\n------------- BEST RESULTS ------------\n')
    fd.write("Best Feature Set: " + "+".join(featset_d.keys()) + "\n")
    fd.write("Accuracy CV-scores: {scr:.3f} %\n\n".format(scr=cv_scores.mean()*100))
    # predict classifier's output
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    fd.write("Detailed classification report for best feature set:\n")
    fd.write(str(classification_report(y_test, y_pred)))
    return data



# Collects all the feature sets represented as CSV files and located in the
# directory path supplied as argument. Returns a list of pandas DataFrames
# where each element represents a feature set in DataFrame format
def collect_feature_sets(features_dir):
    directory = os.fsencode(features_dir)
    featsets_d = {}
    for csv_file in os.listdir(directory):
        filename = os.fsdecode(csv_file)
        if filename.endswith(".csv"):
            # FIXME: replace the .iloc() invocation when fixed
            # it was added to sloppily patch shoddy to_csv()
            # invocations while extracting features
            featset = pd.read_csv(features_dir + filename).iloc[:,1:]
            # correlate each feature-set name with its pandas DataFrame
            featsets_d[os.path.splitext(filename)[0]] = featset
    return featsets_d



def run_experiment(scoring='accuracy', model='xgb', params_file=None, max_rounds=7, n_jobs=1):
    featsets_d = collect_feature_sets(conf['featsets_dir'])
    current_time = time.strftime("_%d.%m.%y_%H%M", time.localtime())
    res_file = conf['results_dir'] + 'res_' + model + current_time
    with open(params_file, 'r') as ymlfile:
        model_params = yaml.load(ymlfile)
    with open(res_file, 'w+') as fd:
        if model == 'rf':
            clf = RandomForestClassifier(**model_params, n_jobs=n_jobs)
        elif model == 'xgb':
            clf = XGBClassifier(**model_params, n_jobs=n_jobs)
        # TODO: support wider range of models (???)
        else:
            sys.exit(0)
        fd.write('----------- HYPERPARAMETERS -----------\n')
        fd.write('Parameters file name: ' + params_file + '\n\n')
        fd.write('Model instance: \n' + str(clf) + '\n\n')
        fd.write('Model params: \n' + str(model_params) + '\n\n\n')
        fd.write('--------------- RESULTS ---------------\n')
        winner = fwd_stepwise_select(featsets_d, clf, scoring, max_rounds, fd)
    return winner


#exp_file = conf['xgbparams_dir'] + 'xgb_params.yaml'
#run_experiment(scoring='accuracy', model='xgb', params_file=exp_file, n_jobs=8, max_rounds=7)

exp_file = conf['rfparams_dir'] + 'rf_params1.yaml'
run_experiment(model='rf', params_file=exp_file, n_jobs=8, max_rounds=7)



