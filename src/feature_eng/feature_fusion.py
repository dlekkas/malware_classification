import sys
sys.path.insert(0, '../helpers')
from util import load_obj, load_config

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier 
from xgboost import XGBClassifier

from collections import OrderedDict
import pandas as pd
import numpy as np
import os

conf = load_config("../../config.yml")


# Forward stepwise selection was chosen as a suboptimal but tractable
# and computationally feasible algorithm to expose redundant features
# and select the optimal ones. Due to its greedy nature, an optimal
# feature selection is not guaranteed.
def fwd_stepwise_select(featsets_d, scoring, max_rounds=1):
    """
    Parameters
    ----------
    featsets_d: dict
        The dictionary, where key holds the feature-set name and value
        contains the DataFrame representing each distinct feature set
    scoring: str
        The scoring metric to evaluate model performance
    max_rounds: int
        Number of maximum rounds of greedy iterations
    
    Returns
    -------
    pandas.core.frame.DataFrame
        The union of features that yielded the highest score based on the  
        scoring metric supplied as argument
    """
    best_features = OrderedDict()
    best_score = -1
    new_score = 0
    while (new_score > best_score) and (max_rounds > 0):
        max_rounds = max_rounds - 1
        added_featset_name = None
        best_score = new_score
        for name, featset in featsets_d.items():
            best_features[name] = featset
            temp = featset_eval(best_features, scoring=scoring, cv=5)
            # remove the added feature set until all are evaluated
            best_features.popitem()
            if temp >= new_score:
                new_score = temp
                added_featset_name = name

        if featsets_d and added_featset_name is not None:
            best_features[added_featset_name] = featsets_d[added_featset_name]
            del featsets_d[added_featset_name]

    # construct pandas DataFrame based on selected feature sets 
    selected_featset = pd.concat(list(best_features.values()), axis=1, sort=False)
    print("Best Feature Set: " + "+".join(list(best_features.keys())))
    print("Best score: %0.3f" % (best_score))
    return selected_featset



def featset_eval(featset_d, scoring='accuracy', cv=5):
    labels = pd.read_csv(conf['train_labels'])
    data = pd.concat([labels] + list(featset_d.values()), axis=1, sort=False)
    y = data['Class']
    X = data.drop(['Class', 'Id'], axis=1)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    # RandomForest was utilized for feature selection due to efficiency and performance
    # TODO: enhance future selection with XGBoost 
    #fusion_clf = RandomForestClassifier(max_features=3, max_depth=90, n_estimators=300,
            #min_samples_leaf=3, bootstrap=True, min_samples_split=8)
    #cv_scores = cross_val_score(fusion_clf, X_train, y_train, cv=cv, scoring=scoring)

    xgb_clf = XGBClassifier(n_estimators=200, learning_rate=0.25, max_depth=2, min_child_weight=2)
    cv_scores = cross_val_score(xgb_clf, X_train, y_train, cv=cv, scoring=scoring)
    result = cv_scores.mean()
    print("Feature Set: " + "+".join(featset_d.keys()))
    print("Accuracy: %0.3f (+/- %0.2f)\n" % (cv_scores.mean(), cv_scores.std() * 2))
    return result



# Collects all the feature sets represented as CSV files and located in the
# directory path supplied as argument. Returns a list of pandas DataFrames
# where each element represents a feature set in DataFrame format
def collect_feature_sets(features_dir):
    directory = os.fsencode(features_dir)
    featsets_d = {}
    for csv_file in os.listdir(directory):
        filename = os.fsdecode(csv_file)
        if filename.endswith(".csv"):
            # FIXME: replace the .iloc() invocation when fixed
            # it was added to sloppily patch shoddy to_csv()
            # invocations while extracting features
            featset = pd.read_csv(features_dir + filename).iloc[:,1:]
            # correlate each feature-set name with its pandas DataFrame
            featsets_d[os.path.splitext(filename)[0]] = featset
    return featsets_d


scoring = 'accuracy'
featsets_d = collect_feature_sets(conf['featsets_dir'])
winner = fwd_stepwise_select(featsets_d, scoring, 3)

