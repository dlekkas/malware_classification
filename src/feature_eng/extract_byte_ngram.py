from util import save_obj, log_exception, progress_bar, parse_bytes

from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

import binascii
import collections
import pandas as pd
import numpy as np
import logging
import sys
import gzip

DATASET_DIR = '/media/dimlek/TOSHIBA EXT/malware/train/'


def count_byte_ngrams(byte_seq, n):
    ngram_cnt_d = collections.defaultdict(int)
    for i in range(len(byte_seq)):
        if len(byte_seq) < i + n:
            break
        ngram = byte_seq[i:(i+n)] 
        # convert bytearray to hex
        ngram = binascii.hexlify(bytearray(ngram))
        ngram_cnt_d[ngram] += 1
    return ngram_cnt_d


def extract_byte_ngram_features(files_list, n, addrlength=32):
    ngram_d_list = []
    for idx, file_name in enumerate(files_list):
        byte_file = DATASET_DIR + file_name + '.bytes.gz'
        byte_seq = parse_bytes(byte_file, addrlength)
        # check for successful byte file parsing
        if not byte_seq:
            continue
        ngram_cnt_d = count_byte_ngrams(byte_seq, n)
        ngram_d_list.append(ngram_cnt_d)
        progress_bar(idx+1, len(files_list), 50)

    # convert list of dictionaries to a byte ngram count numpy array
    vec = DictVectorizer()
    ngram_freq = vec.fit_transform(ngram_d_list).toarray()
    ngram_freq_df = pd.DataFrame(ngram_freq, columns = vec.get_feature_names())
    # store frequency of each byte ngram
    ngram_freq_df.to_csv('features/' + str(n) + 'gram_byte_freq2.csv', index=False)

    # transform ngram frequency array to ngram tfidf array
    transformer = TfidfTransformer(smooth_idf=False)
    ngram_tfidf = transformer.fit_transform(ngram_freq)
    # store tfidf of each byte ngram in CSV file
    ngram_tfidf_df = pd.DataFrame(ngram_tfidf.todense(), columns=vec.get_feature_names())
    ngram_tfidf_df.to_csv('features/' + str(n) + 'gram_byte_tfidf2.csv', index=False)
    return ngram_freq_df, ngram_tfidf_df


def byte_ngram(files_list, addrlength=32, n=1):
    dicts_list = []
    total_files = len(files_list)
    bad_files_names = []
    for idx, file_name in enumerate(files_list):
        bytes_file = DATASET_DIR + file_name + '.bytes.gz'
        try:
            with gzip.open(bytes_file, 'rt') as fp:
                bytedict = {}
                hex_seq = ""
                for line in fp.readlines():
                    if not line.strip():
                        continue
                    else:
                        address = int(addrlength/4) # hex to bytes
                        # ensure that addresses values will not be counted
                        # in the ngram calculation
                        hex_seq = hex_seq + line[address:].strip()

                hex_seq = hex_seq.replace(" ", "")
                for i in range(0, len(hex_seq)-1, 2):
                    # ignore bytes that contain the "?" character
                    if hex_seq[i] == "?" or hex_seq[i+1] == "?":
                        continue
                    if 2*n+i > len(hex_seq):
                        break
                
                    gram = hex_seq[i:(2*n+i)]
                    if gram not in bytedict.keys():
                        bytedict[gram] = 1
                    else:
                        bytedict[gram] += 1
                
                dicts_list.append(bytedict)
        except Exception as e:
            bad_files_names.append(file_name)
            log_exception(e, sys.argv[0], bytes_file)

        # progress bars always save my sanity
        progress_bar(idx+1, total_files, 50)
    
    # log the corrupted files for future reference
    if len(bad_files_names) > 0:
        with open('bad_bytes_files.txt', 'w') as bfp:
            for name in bad_files_names:
                bfp.write(name + '.bytes\n')

    # convert list of dictionaries to a byte ngram count numpy array
    vec = DictVectorizer()
    ngram_freq = vec.fit_transform(dicts_list).toarray()
    ngram_freq_df = pd.DataFrame(ngram_freq, columns = vec.get_feature_names())
    # store frequency of each byte ngram
    ngram_freq_df.to_csv('features/' + str(n) + 'gram_byte_freq.csv')
    save_obj(ngram_freq_df, str(n) + 'gram_byte_freq')

    # transform ngram frequency array to ngram tfidf array
    transformer = TfidfTransformer(smooth_idf=False)
    ngram_tfidf = transformer.fit_transform(ngram_freq)
    # store tfidf of each byte ngram 
    ngram_tfidf_df = pd.DataFrame(ngram_tfidf.todense(), columns=vec.get_feature_names())
    ngram_tfidf_df.to_csv('features/' + str(n) + 'gram_byte_tfidf.csv')
    save_obj(ngram_tfidf_df, str(n) + 'gram_byte_tfidf')
    return ngram_tfidf_df



def main():
    train_labels = pd.read_csv('/home/dimlek/Documents/thesis/dataset/dataSample/trainLabels.csv')
    files_list = train_labels['Id'].tolist()
    # calculate TF*IDF of each byte
    extract_byte_ngram_features(files_list[:5], 1)

if __name__ == '__main__':
    main()

