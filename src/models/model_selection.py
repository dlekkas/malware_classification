from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MaxAbsScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.tree import DecisionTreeClassifier 
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from xgboost.sklearn import XGBRegressor

import pandas as pd
import numpy as np
import time
import sys
import os


sys.path.insert(0, '../helpers')
from util import *

conf = load_config("../../config.yml")


# Returns a classifier object based on the name of the classifier given as parameter
def classifier_selection(name):
    if name == 'svm':
        clf = SVC(class_weight={5:30})
    elif name == 'rf':
        clf = RandomForestClassifier()
    elif name == 'dt':
        clf = DecisionTreeClassifier()
    elif name == 'xgb':
        #clf = XGBRegressor()
        clf = XGBClassifier()
    else:
        clf = None
        print('Unsupported Classifier: ' + clf)
    return clf 



def superparam_selection(fd, model, clf_params, X_train, X_test, y_train, y_test, nfolds=5, 
        score_evals= ['accuracy']):   # ['accuracy', 'precision', 'recall', 'f1-micro']):
    bestscore_dict = {}
    clf = classifier_selection(model)
    for score in score_evals:
        grid_search_clf = GridSearchCV(clf, clf_params, cv=nfolds, scoring=score, error_score=0, n_jobs=15, verbose=1)
        grid_search_clf.fit(X_train, y_train)
        fd.write('Best parameters found based on training set: ')
        fd.write(str(grid_search_clf.best_params_) + '\n\n')
        print(grid_search_clf.best_params_)
        bestscore_dict[score] = grid_search_clf.best_params_
        mean_scores = grid_search_clf.cv_results_['mean_test_score']
        std_scores = grid_search_clf.cv_results_['std_test_score']
        print(mean_scores)
        fd.write('Grid scores on training set:\n')
        for mean, std, params in zip(mean_scores, std_scores, grid_search_clf.cv_results_['params']):
            fd.write("%0.3f (+/-%0.03f) for %r\n" % (mean, std * 2, params))

        # predict classifier's output
        fd.write('\nDetailed classification report:\n')
        y_pred = grid_search_clf.predict(X_test)
        fd.write(str(classification_report(y_test, y_pred)))
        fd.close() 
    return bestscore_dict


def create_results_file(model, scaler, pca, score_evals):
    # create a file to store results and related info pertaining to the model 
    curr_time = time.strftime("_%d.%m.%y_%H%M", time.localtime())
    res_file = conf['results_dir'] + 'res_' + model + curr_time
    with open(res_file, 'w+') as fd:
        fd.write('-' * 44)
        fd.write('\n\nPreprocessing: %s, %s\n' % (scaler, pca)) 
        fd.write('Classifier: %s\n' % model)
        fd.write('Scoring evaluators: %s\n\n' % ",".join(score_evals))
        fd.write('-' * 44)
        fd.write('\n\n')
        return fd

def xgboost_hyperparam():
    pass    




def scale_data(scaler_name, X_train, X_test):
    if scaler_name is None:
        return X_train, X_test

    if scaler_name == 'StandardScaler':
        scaler = StandardScaler()
    elif scaler_name == 'RobustScaler':
        scaler = RobustScaler()
    elif scaler_name == 'MinMaxScaler':
        scaler = MinMaxScaler()
    elif scaler_name == 'MaxAbsScaler':
        scaler = MaxAbsScaler()
    else:
        print('Unsupported scaler ' + scaler_name)
    scaler.fit(X_train)
    X_train = scaler.transform(X_train)
    X_test = scaler.transform(X_test)
    return X_train, X_test


def data_preprocess(dataset_df, scaler_name):
    y = dataset_df['Class']
    X = dataset_df.drop(['Class', 'Id'], axis = 1)
    # ensure consistent split of data with random_state (output verification)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    X_train, X_test = scale_data(scaler_name, X_train, X_test)

    # Applying PCA for dimensionality reduction
    #pca = PCA(0.97)
    #pca.fit(X_train)
    #X_train = pca.transform(X_train)
    #X_test = pca.transform(X_test)

    return X_train, X_test, y_train, y_test
    

def test_pipeline(scaler, model, param_grid, score_evals):
    dataset_df = load_obj("final_data")
    X_train, X_test, y_train, y_test = data_preprocess(dataset_df, scaler)
    curr_time = time.strftime("_%d.%m.%y_%H%M", time.localtime())
    res_file = conf['results_dir'] + 'res_' + model + curr_time
    with open(res_file, 'w+') as fd:
        fd.write('-' * 44)
        fd.write('\n\nPreprocessing: %s, %s\n' % (scaler, 'No PCA')) 
        fd.write('Classifier: %s\n' % model)
        fd.write('Scoring evaluators: %s\n\n' % ",".join(score_evals))
        fd.write('-' * 44)
        fd.write('\n\n')
        superparam_selection(fd, model, param_grid, X_train, X_test, y_train, y_test) #, score_evals)
    


rf_param_grid = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110],
    'max_features': [2, 3],
    'min_samples_leaf': [3, 4, 5],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [100, 200, 300, 1000] }

svm_param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': [0.001, 0.01, 0.1, 1, 10] }

xgb_param_grid = {
    'n_estimators': [200],
    'learning_rate': [0.25],
    'max_depth': [2],
    'min_child_weight': [2]}

dt_param_grid = {
    'max_depth': [50, 100, 200],
    'min_samples_split': [5, 10, 20, 40, 60, 80],
    'min_samples_leaf': [0.1, 1, 10],
    'max_features': [2, 5, 10, 20, 50]}

score_evals = ['accuracy'] #, 'precision', 'recall', 'f1_micro']

#test_pipeline('StandardScaler', 'svm', svm_param_grid, score_evals)
#test_pipeline(None, 'dt', dt_param_grid, score_evals)
#test_pipeline(None, 'rf', rf_param_grid, score_evals)
test_pipeline(None, 'xgb', xgb_param_grid, score_evals)
#test_pipeline(None, 'dt', dt_param_grid, score_evals)

