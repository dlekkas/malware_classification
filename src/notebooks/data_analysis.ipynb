{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the dataset that resulted from feature extraction process\n",
    "\n",
    "Dataset citation: https://arxiv.org/abs/1802.10135\n",
    "\n",
    "* First, we examine how balanced is the dataset provided from Microsoft Kaggle Competition (cited above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Malware Categories Frequency -----\n",
      "\n",
      "Malware Class\t|\tFrequecy\n",
      "0\t\t\t1541\n",
      "1\t\t\t2477\n",
      "2\t\t\t2942\n",
      "3\t\t\t475\n",
      "4\t\t\t42\n",
      "5\t\t\t751\n",
      "6\t\t\t398\n",
      "7\t\t\t1228\n",
      "8\t\t\t1013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Frequency')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util import load_obj\n",
    "data = load_obj('interim_data')\n",
    "\n",
    "# More accurate presentation of occurence of each class\n",
    "print('\\n----- Malware Categories Frequency -----\\n')\n",
    "print('Malware Class\\t|\\tFrequecy')\n",
    "for key, value in enumerate(data['Class'].value_counts().sort_index()):\n",
    "   print(str(key) + '\\t\\t\\t' + str(value))\n",
    "\n",
    "\n",
    "# assuming interim_data has resulted from feature extraction\n",
    "plt.hist(data['Class'])\n",
    "plt.title('Class Distribution of Training Set')\n",
    "plt.xlabel('Malware category')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standardization\n",
    "\n",
    "Since standardization of datasets is a common requirement for many machine learning estimators, I will use scikit-learn to conduct the required preprocessing to prepare the dataset for SVM. Standard scaling will be applied as an initial starting point to generate first results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['Class']\n",
    "X = data.drop(['Class', 'Id'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# create training and testing vars (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardizing the features\n",
    "std_scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = std_scaler.transform(X_train)\n",
    "X_test = std_scaler.transform(X_test)\n",
    "\n",
    "# Applying PCA for dimensionality reduction\n",
    "\n",
    "n = 400\n",
    "pca = PCA(n_components=n)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Results of kaggle competition are evaluated using the logloss\n",
    "# metric, hence this score metric will be used for gridsearchcv \n",
    "def svm_superparam_selection(X_train, X_test, y_train, y_test, nfolds, score_evals):\n",
    "    tuned_params = {\n",
    "            'C': [0.1, 0.01, 0.1, 1, 10],\n",
    "            'kernel': ['rbf', 'sigmoid', 'poly'],\n",
    "            'gamma': [0.1, 0.01, 0.1, 1]}\n",
    "    bestscore_dict = {}\n",
    "    for score in score_evals:\n",
    "        grid_search_clf = GridSearchCV(SVC(), tuned_params, cv=nfolds, scoring=score)\n",
    "        grid_search_clf.fit(X_train, y_train)\n",
    "        print('# Tuning hyper-parameters for %s' % score)\n",
    "        print('Best parameters found based on training set')\n",
    "        print(grid_search_clf.best_params_)\n",
    "        bestscore_dict[score] = grid_search_clf.best_params_\n",
    "        mean_scores = grid_search_clf.cv_results_['mean_test_score']\n",
    "        std_scores = grid_search_clf.cv_results_['std_test_score']\n",
    "        print('Grid scores on training set:')\n",
    "        for mean, std, params in zip(mean_scores, std_scores, grid_search_clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "        # predict classifier's output\n",
    "        print('Detailed classification report:')\n",
    "        print('Scores based on full test set')\n",
    "        y_pred = grid_search_clf.predict(X_test)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return bestscore_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "Best parameters found based on training set\n",
      "{'C': 0.1, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "Grid scores on training set:\n",
      "0.786 (+/-0.010) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.583 (+/-0.031) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'sigmoid'}\n",
      "0.964 (+/-0.002) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.840 (+/-0.009) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.855 (+/-0.016) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.801 (+/-0.017) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.786 (+/-0.010) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.583 (+/-0.031) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'sigmoid'}\n",
      "0.964 (+/-0.002) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.689 (+/-0.013) for {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.537 (+/-0.039) for {'C': 0.1, 'gamma': 1, 'kernel': 'sigmoid'}\n",
      "0.956 (+/-0.004) for {'C': 0.1, 'gamma': 1, 'kernel': 'poly'}\n",
      "0.537 (+/-0.012) for {'C': 0.01, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.613 (+/-0.008) for {'C': 0.01, 'gamma': 0.1, 'kernel': 'sigmoid'}\n",
      "0.955 (+/-0.008) for {'C': 0.01, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.696 (+/-0.012) for {'C': 0.01, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.758 (+/-0.009) for {'C': 0.01, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.597 (+/-0.015) for {'C': 0.01, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.537 (+/-0.012) for {'C': 0.01, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.613 (+/-0.008) for {'C': 0.01, 'gamma': 0.1, 'kernel': 'sigmoid'}\n",
      "0.955 (+/-0.008) for {'C': 0.01, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.515 (+/-0.010) for {'C': 0.01, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.538 (+/-0.034) for {'C': 0.01, 'gamma': 1, 'kernel': 'sigmoid'}\n",
      "0.963 (+/-0.002) for {'C': 0.01, 'gamma': 1, 'kernel': 'poly'}\n",
      "0.786 (+/-0.010) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.583 (+/-0.031) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'sigmoid'}\n",
      "0.964 (+/-0.002) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.840 (+/-0.009) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.855 (+/-0.016) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.801 (+/-0.017) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.786 (+/-0.010) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.583 (+/-0.031) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'sigmoid'}\n",
      "0.964 (+/-0.002) for {'C': 0.1, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.689 (+/-0.013) for {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.537 (+/-0.039) for {'C': 0.1, 'gamma': 1, 'kernel': 'sigmoid'}\n",
      "0.956 (+/-0.004) for {'C': 0.1, 'gamma': 1, 'kernel': 'poly'}\n",
      "0.875 (+/-0.007) for {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.597 (+/-0.040) for {'C': 1, 'gamma': 0.1, 'kernel': 'sigmoid'}\n",
      "0.964 (+/-0.004) for {'C': 1, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.914 (+/-0.007) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.853 (+/-0.014) for {'C': 1, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.918 (+/-0.007) for {'C': 1, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.875 (+/-0.007) for {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.597 (+/-0.040) for {'C': 1, 'gamma': 0.1, 'kernel': 'sigmoid'}\n",
      "0.964 (+/-0.004) for {'C': 1, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.782 (+/-0.006) for {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.512 (+/-0.052) for {'C': 1, 'gamma': 1, 'kernel': 'sigmoid'}\n",
      "0.951 (+/-0.010) for {'C': 1, 'gamma': 1, 'kernel': 'poly'}\n",
      "0.882 (+/-0.007) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.571 (+/-0.044) for {'C': 10, 'gamma': 0.1, 'kernel': 'sigmoid'}\n",
      "0.963 (+/-0.002) for {'C': 10, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.925 (+/-0.009) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.834 (+/-0.017) for {'C': 10, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.955 (+/-0.008) for {'C': 10, 'gamma': 0.01, 'kernel': 'poly'}\n",
      "0.882 (+/-0.007) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.571 (+/-0.044) for {'C': 10, 'gamma': 0.1, 'kernel': 'sigmoid'}\n",
      "0.963 (+/-0.002) for {'C': 10, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "0.789 (+/-0.010) for {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.496 (+/-0.055) for {'C': 10, 'gamma': 1, 'kernel': 'sigmoid'}\n",
      "0.949 (+/-0.009) for {'C': 10, 'gamma': 1, 'kernel': 'poly'}\n",
      "Detailed classification report:\n",
      "Scores based on full test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.97      0.92      0.94       293\n",
      "          2       0.98      0.99      0.99       529\n",
      "          3       1.00      0.99      1.00       588\n",
      "          4       0.85      0.96      0.90        85\n",
      "          5       0.57      0.50      0.53         8\n",
      "          6       0.95      0.93      0.94       142\n",
      "          7       1.00      1.00      1.00        70\n",
      "          8       0.96      0.94      0.95       254\n",
      "          9       0.96      0.99      0.98       205\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2174\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': {'C': 0.1, 'gamma': 0.1, 'kernel': 'poly'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfolds = 5\n",
    "scoring_evaluators = ['accuracy']\n",
    "svm_superparam_selection(X_train, X_test, y_train, y_test, nfolds, scoring_evaluators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
